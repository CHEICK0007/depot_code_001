# PS: prototype fait pour tourner en local
"""
DESCRIPTION:
c'est ce qu'on appel un "EVI" (Employ√© Virtuel Intelligent).
    d√©ployer sur un server et connect√© & connect√© par un infrastructure telephonique (astersik), 
 il travail h24  << pour vous >> en automatisant une partie de votre activ√© ( proffessionnelle, informelle....)

    
   I) - vous fourniss√© toutes les infos sur votre activit√© 
                       |||
   II) - ia les enregistre par un la methode du RAG
                       |||
   III) - et vous avez maintenant une personne capable de ventre, promouvoir, discuter, informer avec n'importe qui en fonction de vos besoin          




                    C'EST CA L'AFFIQUE DEDAIN, C' EST L'EVI !!!! 
"""

import queue
import json
import sounddevice as sd
from vosk import Model, KaldiRecognizer

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter


# comfigurattion

# üìÑ Charge un document (ex: fichier JSON ou texte)
loader = TextLoader("restaurant_info.json", encoding="utf-8")
documents = loader.load()

# ‚úÇÔ∏è D√©coupe le texte en morceaux plus courts
text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# üß† Embeddings + indexation
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(docs, embedding)

# üîç Construction de la cha√Æne RAG
retriever = vectorstore.as_retriever()
llm = Ollama(model="gemma3:4b")                               "LE MODEL UTILISER EST PETIT POUR DES RAISON DE MANQUE DE PUISSANCE "



# Template & RAG

""" On va prendre l'exemple d'un restorant vantant automatiser sa comunication 
              on construit donc un emply√© (EVI) pour √ßa """

prompt_template1 = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Tu es un assistant de restaurant qui ne salue JAMAIS. R√©ponds strictement selon ces r√®gles :

1. **Sources** : Utilise uniquement ces donn√©es :
{context}

2. **Format** : 
- Pas de demande d'informations personnelles

3. **Restrictions** :

- Jamais de sites web/num√©ros de t√©l√©phone
- Jamais de "veuillez contacter..." ou "vous pouvez trouver..."
- ne donne pas de r√©ponses si tu n'en n'as pas 
Question : {question}
R√©ponse :
"""
)

qa_chain1 = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template1}
)


# discussion Vocal

# üíæ Initialise un historique de conversation
conversation_history = ""
q = queue.Queue()
def callback(indata, frames, time, status):
    if status:
        print("‚ö†Ô∏è", status)
    q.put(bytes(indata))

model = Model(r"vosk-model-small-fr-0.22") # Un model de convertion voice_to_text
recognizer = KaldiRecognizer(model, 16000)

print("üéôÔ∏è Assistant en √©coute... (Ctrl+C pour arr√™ter)")

with sd.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16',
                       channels=1, callback=callback):
    try:
        info_dis = []
        while True:
            data = q.get()
            if recognizer.AcceptWaveform(data):
                result = json.loads(recognizer.Result())
                phrase = result.get("text", "").strip()
                if phrase:
                    print("üó£Ô∏è Question :", phrase)
                    info_dis.append(phrase)

                    # üîÅ On construit le prompt avec l'historique
                    full_prompt = f"""
                    [Historique de la conversation jusqu'√† pr√©sent]
                    {conversation_history}

                    Maintenant, r√©ponds √† SEULEMENT et cette nouvelle question de l'utilisateur :

                    [Nouvelle question]
                    {phrase}
                    """
                    # üß† Requ√™te au mod√®le
                    response = qa_chain1.run(full_prompt)

                    # üîÇ Ajoute l'√©change √† l'historique pour cr√©er une cha√Æne de questions-r√©ponses (conversation)
                    conversation_history += f"\nUtilisateur : {phrase}\nAssistant : {response}"

                    print("ü§ñ R√©ponse :", response)
                    lire_texte(response)

    except KeyboardInterrupt:
     print("\nüõë Assistant arr√™t√©.")
